{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWV8oes-wKR"
   },
   "source": [
    "# COURSE: A deep understanding of deep learning\n",
    "## SECTION: ANNs\n",
    "### LECTURE: Model depth vs. breadth\n",
    "#### TEACHER: Mike X Cohen, sincxpress.com\n",
    "##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202305"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3870,
     "status": "ok",
     "timestamp": 1676840552227,
     "user": {
      "displayName": "Mike X Cohen",
      "userId": "13901636194183843661"
     },
     "user_tz": -540
    },
    "id": "YeuAheYyhdZw"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ViJutqaaNb2"
   },
   "source": [
    "# Import and organize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4553,
     "status": "ok",
     "timestamp": 1676840556775,
     "user": {
      "displayName": "Mike X Cohen",
      "userId": "13901636194183843661"
     },
     "user_tz": -540
    },
    "id": "MU7rvmWuhjud"
   },
   "outputs": [],
   "source": [
    "MNIST = torchvision.datasets.MNIST(\".\", download=True)\n",
    "data = MNIST.data\n",
    "labels = MNIST.targets\n",
    "\n",
    "# Randomly drop samples to shrink the size to 20,000\n",
    "np.random.seed(42)  # Set random seed for reproducibility\n",
    "indices = np.random.choice(len(data), size=20000, replace=False)\n",
    "data = data[:20000]\n",
    "labels = labels[:20000]\n",
    "\n",
    "# Reshape data to 2D array\n",
    "data = data.reshape(data.shape[0], -1)\n",
    "# Reshape labels to 2D array\n",
    "labels = labels.reshape(labels.shape[0], -1)\n",
    "\n",
    "data_norm = data / torch.max(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: convert to tensor\n",
    "dataT = data_norm.clone().detach().float()\n",
    "labelsT = labels.clone().detach().long()  # long = int64\n",
    "\n",
    "# Step 2: use scikitlearn to split the data\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    dataT, labelsT, test_size=.1)\n",
    "\n",
    "# Step 3: convert into PyTorch Datasets\n",
    "train_data = TensorDataset(train_data, train_labels)\n",
    "test_data = TensorDataset(test_data, test_labels)\n",
    "\n",
    "# Step 4: translate into dataloader objects\n",
    "batchsize = 32\n",
    "train_loader_norm = DataLoader(train_data,\n",
    "                               batch_size=batchsize,\n",
    "                               shuffle=True,\n",
    "                               drop_last=True)\n",
    "test_loader_norm = DataLoader(test_data,\n",
    "                              batch_size=test_data.tensors[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Training data NORM range from {torch.min(train_data.tensors[0])} to {torch.max(train_data.tensors[0])}\"\n",
    ")\n",
    "print(\n",
    "    f\"Test data NORM range from {torch.min(test_data.tensors[0])} to {torch.max(test_data.tensors[0])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCuMSE6baRar"
   },
   "source": [
    "# Construct and sanity-check the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1676840556776,
     "user": {
      "displayName": "Mike X Cohen",
      "userId": "13901636194183843661"
     },
     "user_tz": -540
    },
    "id": "eZMzMLxfULjf"
   },
   "outputs": [],
   "source": [
    "# create a class for the model\n",
    "def createTheMNISTNet(num_hidden_layers, num_hidden_units):\n",
    "\n",
    "    class mnistNet(nn.Module):\n",
    "\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            ### input layer\n",
    "            self.input = nn.Linear(784, num_hidden_units)\n",
    "\n",
    "            ### hidden layer\n",
    "            self.fc_layers = [\n",
    "                nn.Linear(num_hidden_units, num_hidden_units)\n",
    "                for i in range(0, num_hidden_layers)\n",
    "            ]\n",
    "\n",
    "            ### output layer\n",
    "            self.output = nn.Linear(num_hidden_units, 10)\n",
    "\n",
    "        # forward pass\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.input(x))\n",
    "            for hidden_layer in self.fc_layers:\n",
    "                x = F.relu(hidden_layer(x))\n",
    "            return self.output(x)\n",
    "\n",
    "    # create the model instance\n",
    "    net = mnistNet()\n",
    "\n",
    "    # loss function\n",
    "    lossfun = nn.CrossEntropyLoss()\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=.01)\n",
    "\n",
    "    return net, lossfun, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YL7cvyjUaXjc"
   },
   "source": [
    "# Create a function that trains the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1676840557651,
     "user": {
      "displayName": "Mike X Cohen",
      "userId": "13901636194183843661"
     },
     "user_tz": -540
    },
    "id": "cVD1nFTli7TO"
   },
   "outputs": [],
   "source": [
    "# a function that trains the model\n",
    "\n",
    "\n",
    "def function2trainTheModel(train_loader,\n",
    "                           test_loader,\n",
    "                           numepochs,\n",
    "                           num_hidden_layers=1,\n",
    "                           num_hidden_units=50):\n",
    "\n",
    "    # create a new model\n",
    "    net, lossfun, optimizer = createTheMNISTNet(num_hidden_layers,\n",
    "                                                num_hidden_units)\n",
    "\n",
    "    # initialize losses\n",
    "    losses = torch.zeros(numepochs)\n",
    "    trainAcc = []\n",
    "    testAcc = []\n",
    "\n",
    "    # loop over epochs\n",
    "    for epochi in range(numepochs):\n",
    "\n",
    "        # loop over training data batches\n",
    "        batchAcc = []\n",
    "        batchLoss = []\n",
    "        for X, y in train_loader:\n",
    "            y = torch.flatten(y)\n",
    "\n",
    "            # forward pass and loss\n",
    "            yHat = net(X)\n",
    "            loss = lossfun(yHat, y)\n",
    "\n",
    "            # backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # loss from this batch\n",
    "            batchLoss.append(loss.item())\n",
    "\n",
    "            # compute accuracy\n",
    "            matches = torch.argmax(yHat, axis=1) == y  # booleans (false/true)\n",
    "            matchesNumeric = matches.float()  # convert to numbers (0/1)\n",
    "            accuracyPct = 100 * torch.mean(matchesNumeric)  # average and x100\n",
    "            batchAcc.append(accuracyPct)  # add to list of accuracies\n",
    "        # end of batch loop...\n",
    "\n",
    "        # now that we've trained through the batches, get their average training accuracy\n",
    "        trainAcc.append(np.mean(batchAcc))\n",
    "\n",
    "        # and get average losses across the batches\n",
    "        losses[epochi] = np.mean(batchLoss)\n",
    "\n",
    "        # test accuracy\n",
    "        X, y = next(iter(test_loader))  # extract X,y from test dataloader\n",
    "        with torch.no_grad():\n",
    "            y = torch.flatten(y)\n",
    "            yHat = net(X)\n",
    "\n",
    "        # compare the following really long line of code to the training accuracy lines\n",
    "        testAcc.append(100 * torch.mean(\n",
    "            (torch.argmax(yHat, axis=1) == y).float()))\n",
    "\n",
    "    # end epochs\n",
    "\n",
    "    # function output\n",
    "    return trainAcc, testAcc, losses, net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs\n",
    "numepochs = 60\n",
    "accuracy_experiments = {}\n",
    "\n",
    "for num_layers in range(1, 4):\n",
    "    accuracy_experiments[num_layers] = {}\n",
    "    for num_units in range(50, 251, 50):\n",
    "        print(\n",
    "            f\"Training with {num_layers} hidden layers and {num_units} hidden units\"\n",
    "        )\n",
    "        train_accuracy_experiment, test_accuracy_experiment, losses_experiment, net_experiment = function2trainTheModel(\n",
    "            train_loader_norm, test_loader_norm, numepochs, num_layers,\n",
    "            num_units)\n",
    "        accuracy_experiments[num_layers][num_units] = {\n",
    "            \"train_accuracy_experiment\": train_accuracy_experiment,\n",
    "            \"test_accuracy_experiment\": test_accuracy_experiment\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 2, figsize=(16, 25))\n",
    "\n",
    "for i, num_units in zip(range(0, 5), range(50, 251, 50)):\n",
    "    ax[i][0].plot(\n",
    "        accuracy_experiments[1][num_units][\"train_accuracy_experiment\"],\n",
    "        label='1')\n",
    "    ax[i][0].plot(\n",
    "        accuracy_experiments[2][num_units][\"train_accuracy_experiment\"],\n",
    "        label='2')\n",
    "    ax[i][0].plot(\n",
    "        accuracy_experiments[3][num_units][\"train_accuracy_experiment\"],\n",
    "        label='3')\n",
    "    ax[i][0].set_xlabel('Number of hidden units')\n",
    "    ax[i][0].set_ylabel('Accuracy (%)')\n",
    "    ax[i][0].set_ylim([0, 100])\n",
    "    ax[i][0].set_title('Train')\n",
    "    ax[i][0].legend()\n",
    "\n",
    "    ax[i][1].plot(\n",
    "        accuracy_experiments[1][num_units][\"test_accuracy_experiment\"],\n",
    "        label='1')\n",
    "    ax[i][1].plot(\n",
    "        accuracy_experiments[2][num_units][\"test_accuracy_experiment\"],\n",
    "        label='2')\n",
    "    ax[i][1].plot(\n",
    "        accuracy_experiments[3][num_units][\"test_accuracy_experiment\"],\n",
    "        label='3')\n",
    "    ax[i][1].set_xlabel('Number of hidden units')\n",
    "    ax[i][1].set_ylabel('Accuracy (%)')\n",
    "    ax[i][1].set_ylim([0, 100])\n",
    "    ax[i][1].set_title(\"Test\")\n",
    "    ax[i][1].legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN4056RCJOTpz/xZ+H60BLF",
   "provenance": [
    {
     "file_id": "1V0mkFVCMTAd0iq30FIpXspezzE7PpVrX",
     "timestamp": 1616523401452
    },
    {
     "file_id": "1YpHocGI4rApOxIBb1ZghCU5L-hFnv4CK",
     "timestamp": 1615925514977
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
